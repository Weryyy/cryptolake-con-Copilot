# ============================================================
# CryptoLake — Docker Compose
# ============================================================
# Levanta todo el ecosistema de data engineering con:
#   docker compose up -d
#
# Servicios:
#   - MinIO (S3 local)        → Puerto 9000 (API), 9001 (Console)
#   - Iceberg REST Catalog    → Puerto 8181
#   - Kafka (KRaft mode)      → Puerto 9092
#   - Kafka UI                → Puerto 8080
#   - Spark Master + Worker   → Puerto 8082 (UI), 7077 (master)
#   - Airflow                 → Puerto 8083
#   - PostgreSQL (para Airflow)
# ============================================================

x-common-env: &common-env
  MINIO_ENDPOINT: http://minio:9000
  MINIO_ACCESS_KEY: cryptolake
  MINIO_SECRET_KEY: cryptolake123
  KAFKA_BOOTSTRAP_SERVERS: kafka:29092
  ICEBERG_CATALOG_URI: http://iceberg-rest:8181
  AWS_ACCESS_KEY_ID: cryptolake
  AWS_SECRET_ACCESS_KEY: cryptolake123
  AWS_REGION: us-east-1

services:

  # ==========================================================
  # CAPA DE ALMACENAMIENTO
  # ==========================================================

  minio:
    image: minio/minio:latest
    container_name: cryptolake-minio
    ports:
      - "9000:9000"
      - "9001:9001"
    environment:
      MINIO_ROOT_USER: cryptolake
      MINIO_ROOT_PASSWORD: cryptolake123
    command: server /data --console-address ":9001"
    volumes:
      - minio-data:/data
    healthcheck:
      test: ["CMD", "mc", "ready", "local"]
      interval: 10s
      timeout: 5s
      retries: 5

  minio-init:
    image: minio/mc:latest
    container_name: cryptolake-minio-init
    depends_on:
      minio:
        condition: service_healthy
    entrypoint: >
      /bin/sh -c "
      mc alias set local http://minio:9000 cryptolake cryptolake123;
      mc mb local/cryptolake-bronze --ignore-existing;
      mc mb local/cryptolake-silver --ignore-existing;
      mc mb local/cryptolake-gold --ignore-existing;
      mc mb local/cryptolake-checkpoints --ignore-existing;
      echo 'Buckets created successfully';
      "

  iceberg-rest:
    image: tabulario/iceberg-rest:1.5.0
    container_name: cryptolake-iceberg-rest
    ports:
      - "8181:8181"
    environment:
      CATALOG_WAREHOUSE: s3://cryptolake-bronze/
      CATALOG_IO__IMPL: org.apache.iceberg.aws.s3.S3FileIO
      CATALOG_S3_ENDPOINT: http://minio:9000
      CATALOG_S3_PATH__STYLE__ACCESS: "true"
      AWS_ACCESS_KEY_ID: cryptolake
      AWS_SECRET_ACCESS_KEY: cryptolake123
      AWS_REGION: us-east-1
    depends_on:
      minio:
        condition: service_healthy

  # ==========================================================
  # CAPA DE STREAMING
  # ==========================================================

  kafka:
    image: confluentinc/cp-kafka:7.6.0
    container_name: cryptolake-kafka
    ports:
      - "9092:9092"
    environment:
      KAFKA_NODE_ID: 1
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,EXTERNAL:PLAINTEXT
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:29092,EXTERNAL://localhost:9092
      KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:29092,CONTROLLER://0.0.0.0:29093,EXTERNAL://0.0.0.0:9092
      KAFKA_CONTROLLER_LISTENER_NAMES: CONTROLLER
      KAFKA_CONTROLLER_QUORUM_VOTERS: 1@kafka:29093
      KAFKA_PROCESS_ROLES: broker,controller
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
      CLUSTER_ID: MkU3OEVBNTcwNTJENDM2Qk
    volumes:
      - kafka-data:/var/lib/kafka/data
    healthcheck:
      test: kafka-topics --bootstrap-server localhost:29092 --list
      interval: 10s
      timeout: 10s
      retries: 10

  kafka-ui:
    image: provectuslabs/kafka-ui:latest
    container_name: cryptolake-kafka-ui
    ports:
      - "8080:8080"
    environment:
      KAFKA_CLUSTERS_0_NAME: cryptolake
      KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS: kafka:29092
    depends_on:
      kafka:
        condition: service_healthy

  # ==========================================================
  # CAPA DE PROCESAMIENTO
  # ==========================================================

  spark-master:
    build:
      context: ./docker/spark
      dockerfile: Dockerfile
    container_name: cryptolake-spark-master
    ports:
      - "8082:8080"
      - "7077:7077"
    environment:
      <<: *common-env
      SPARK_MODE: master
    volumes:
      - ./src:/opt/spark/work/src

  spark-worker:
    build:
      context: ./docker/spark
      dockerfile: Dockerfile
    container_name: cryptolake-spark-worker
    environment:
      <<: *common-env
      SPARK_MODE: worker
      SPARK_MASTER_URL: spark://spark-master:7077
      SPARK_WORKER_MEMORY: 2g
      SPARK_WORKER_CORES: 2
    depends_on:
      - spark-master
    volumes:
      - ./src:/opt/spark/work/src

  # ==========================================================
  # CAPA DE ORQUESTACIÓN
  # ==========================================================

  airflow-postgres:
    image: postgres:16-alpine
    container_name: cryptolake-airflow-db
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow
    volumes:
      - airflow-db-data:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U airflow"]
      interval: 5s
      timeout: 5s
      retries: 5

  airflow-webserver:
    build:
      context: ./docker/airflow
      dockerfile: Dockerfile
    container_name: cryptolake-airflow-webserver
    ports:
      - "8083:8080"
    environment:
      <<: *common-env
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@airflow-postgres/airflow
      AIRFLOW__CORE__FERNET_KEY: ''
      AIRFLOW__WEBSERVER__SECRET_KEY: cryptolake-secret-key
      AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: 'true'
      AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
    volumes:
      - ./src/orchestration/dags:/opt/airflow/dags
      - ./src:/opt/airflow/src
      - airflow-logs:/opt/airflow/logs
    depends_on:
      airflow-postgres:
        condition: service_healthy
    command: >
      bash -c "
      airflow db migrate &&
      airflow users create
        --username admin
        --password admin
        --firstname Admin
        --lastname User
        --role Admin
        --email admin@cryptolake.dev || true &&
      airflow webserver
      "

  airflow-scheduler:
    build:
      context: ./docker/airflow
      dockerfile: Dockerfile
    container_name: cryptolake-airflow-scheduler
    environment:
      <<: *common-env
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@airflow-postgres/airflow
      AIRFLOW__CORE__FERNET_KEY: ''
      AIRFLOW__WEBSERVER__SECRET_KEY: cryptolake-secret-key
      AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
    volumes:
      - ./src/orchestration/dags:/opt/airflow/dags
      - ./src:/opt/airflow/src
      - airflow-logs:/opt/airflow/logs
    depends_on:
      airflow-postgres:
        condition: service_healthy
    command: airflow scheduler

  # ==========================================================
  # CAPA DE SERVING
  # ==========================================================

  api:
    build:
      context: ./docker/api
      dockerfile: Dockerfile
    container_name: cryptolake-api
    ports:
      - "8000:8000"
    environment:
      <<: *common-env
    volumes:
      - ./src:/app/src

  dashboard:
    image: python:3.11-slim
    container_name: cryptolake-dashboard
    ports:
      - "8501:8501"
    environment:
      <<: *common-env
      API_URL: http://api:8000
    volumes:
      - ./src/serving/dashboard:/app
    command: >
      bash -c "pip install streamlit requests plotly pandas &&
      streamlit run /app/app.py --server.address 0.0.0.0"
    depends_on:
      - api

  # ==========================================================
  # MONITORING
  # ==========================================================

  prometheus:
    image: prom/prometheus:latest
    container_name: cryptolake-prometheus
    ports:
      - "9090:9090"
    volumes:
      - ./docker/prometheus/prometheus.yml:/etc/prometheus/prometheus.yml

  grafana:
    image: grafana/grafana:latest
    container_name: cryptolake-grafana
    ports:
      - "3000:3000"
    environment:
      GF_SECURITY_ADMIN_PASSWORD: cryptolake
    volumes:
      - grafana-data:/var/lib/grafana

volumes:
  minio-data:
  kafka-data:
  airflow-db-data:
  airflow-logs:
  grafana-data:
