# ============================================================
# CryptoLake — Docker Compose
# ============================================================
# Levanta todo el ecosistema de data engineering con:
#   docker compose up -d
#
# Servicios:
#   - MinIO (S3 local)        → Puerto 9000 (API), 9001 (Console)
#   - Iceberg REST Catalog    → Puerto 8181
#   - Kafka (KRaft mode)      → Puerto 9092
#   - Kafka UI                → Puerto 8080
#   - Spark Master + Worker   → Puerto 8082 (UI), 7077 (master)
#   - Airflow                 → Puerto 8083
#   - PostgreSQL (para Airflow)
# ============================================================

x-common-env: &common-env
  MINIO_ENDPOINT: http://minio:9000
  MINIO_ACCESS_KEY: cryptolake
  MINIO_SECRET_KEY: cryptolake123
  KAFKA_BOOTSTRAP_SERVERS: kafka:29092
  ICEBERG_CATALOG_URI: http://cryptolake-iceberg-rest:8181
  AWS_ACCESS_KEY_ID: cryptolake
  AWS_SECRET_ACCESS_KEY: cryptolake123
  AWS_REGION: us-east-1
  REDIS_HOST: redis
  REDIS_PORT: 6379

services:

  redis:
    image: redis:7-alpine
    container_name: cryptolake-redis
    ports:
      - "6379:6379"
    healthcheck:
      test: [ "CMD", "redis-cli", "ping" ]
      interval: 5s
      timeout: 3s
      retries: 5

  # ==========================================================
  # CAPA DE ALMACENAMIENTO
  # ==========================================================

  minio:
    image: minio/minio:latest
    container_name: cryptolake-minio
    ports:
      - "9000:9000"
      - "9001:9001"
    environment:
      MINIO_ROOT_USER: cryptolake
      MINIO_ROOT_PASSWORD: cryptolake123
    command: server /data --console-address ":9001"
    volumes:
      - minio-data:/data
    healthcheck:
      test: [ "CMD", "mc", "ready", "local" ]
      interval: 10s
      timeout: 5s
      retries: 5

  minio-init:
    image: minio/mc:latest
    container_name: cryptolake-minio-init
    depends_on:
      minio:
        condition: service_healthy
    entrypoint: >
      /bin/sh -c " mc alias set local http://minio:9000 cryptolake cryptolake123; mc mb local/cryptolake-bronze --ignore-existing; mc mb local/cryptolake-silver --ignore-existing; mc mb local/cryptolake-gold --ignore-existing; mc mb local/cryptolake-checkpoints --ignore-existing; echo 'Buckets created successfully'; "

  iceberg-rest:
    image: tabulario/iceberg-rest:1.5.0
    container_name: cryptolake-iceberg-rest
    ports:
      - "8181:8181"
    environment:
      CATALOG_WAREHOUSE: s3://cryptolake-bronze/
      CATALOG_IO__IMPL: org.apache.iceberg.aws.s3.S3FileIO
      CATALOG_S3_ENDPOINT: http://minio:9000
      CATALOG_S3_PATH__STYLE__ACCESS: "true"
      AWS_ACCESS_KEY_ID: cryptolake
      AWS_SECRET_ACCESS_KEY: cryptolake123
      AWS_REGION: us-east-1
      # JDBC Catalog configuration to avoid SQLite locking
      CATALOG_CATALOG__IMPL: org.apache.iceberg.jdbc.JdbcCatalog
      CATALOG_URI: jdbc:postgresql://airflow-postgres:5432/airflow
      CATALOG_JDBC_URL: jdbc:postgresql://airflow-postgres:5432/airflow
      CATALOG_JDBC_USER: airflow
      CATALOG_JDBC_PASSWORD: airflow
    depends_on:
      minio:
        condition: service_healthy
      airflow-postgres:
        condition: service_healthy

  # ==========================================================
  # CAPA DE STREAMING
  # ==========================================================

  kafka:
    image: confluentinc/cp-kafka:7.6.0
    container_name: cryptolake-kafka
    ports:
      - "9092:9092"
    environment:
      KAFKA_NODE_ID: 1
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,EXTERNAL:PLAINTEXT
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:29092,EXTERNAL://localhost:9092
      KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:29092,CONTROLLER://0.0.0.0:29093,EXTERNAL://0.0.0.0:9092
      KAFKA_CONTROLLER_LISTENER_NAMES: CONTROLLER
      KAFKA_CONTROLLER_QUORUM_VOTERS: 1@kafka:29093
      KAFKA_PROCESS_ROLES: broker,controller
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
      CLUSTER_ID: MkU3OEVBNTcwNTJENDM2Qk
    volumes:
      - kafka-data:/var/lib/kafka/data
    healthcheck:
      test: kafka-topics --bootstrap-server localhost:29092 --list
      interval: 10s
      timeout: 10s
      retries: 10

  kafka-ui:
    image: provectuslabs/kafka-ui:latest
    container_name: cryptolake-kafka-ui
    ports:
      - "8080:8080"
    environment:
      KAFKA_CLUSTERS_0_NAME: cryptolake
      KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS: kafka:29092
    depends_on:
      kafka:
        condition: service_healthy

  # ==========================================================
  # CAPA DE PROCESAMIENTO
  # ==========================================================

  spark-master:
    build:
      context: ./docker/spark
      dockerfile: Dockerfile
    container_name: cryptolake-spark-master
    ports:
      - "8082:8080"
      - "7077:7077"
      - "10001:10001"
    environment:
      <<: *common-env
      PYTHONPATH: /opt/spark/work
      SPARK_MASTER_HOST: 0.0.0.0
    command: >
      /bin/bash -c " /opt/spark/bin/spark-class org.apache.spark.deploy.master.Master & echo 'Waiting for Spark Master to start...' && sleep 10 && echo 'Starting Thrift Server...' && /opt/spark/sbin/start-thriftserver.sh  --master spark://127.0.0.1:7077  --conf spark.sql.catalog.cryptolake=org.apache.iceberg.spark.SparkCatalog  --conf spark.sql.catalog.cryptolake.type=rest  --conf spark.sql.catalog.cryptolake.uri=http://cryptolake-iceberg-rest:8181  --conf spark.sql.catalog.cryptolake.io-impl=org.apache.iceberg.aws.s3.S3FileIO  --conf spark.sql.catalog.cryptolake.s3.endpoint=http://minio:9000  --conf spark.sql.catalog.cryptolake.s3.path-style-access=true  --conf spark.sql.catalog.cryptolake.s3.access-key-id=cryptolake  --conf spark.sql.catalog.cryptolake.s3.secret-access-key=cryptolake123  --conf spark.sql.extensions=org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions  --conf spark.sql.defaultCatalog=cryptolake  --conf spark.hadoop.hive.server2.thrift.port=10001; sleep infinity "
    volumes:
      - ./src:/opt/spark/work/src

  spark-worker:
    build:
      context: ./docker/spark
      dockerfile: Dockerfile
    container_name: cryptolake-spark-worker
    environment:
      <<: *common-env
      SPARK_MASTER_URL: spark://spark-master:7077
      PYTHONPATH: /opt/spark/work
      SPARK_WORKER_MEMORY: 4g
      SPARK_WORKER_CORES: 4
    command: /opt/spark/bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077
    depends_on:
      - spark-master
    volumes:
      - ./src:/opt/spark/work/src

  # ==========================================================
  # REAL-TIME ANALYZERS
  # ==========================================================

  streaming-producer:
    build:
      context: ./docker/api
      dockerfile: Dockerfile
    container_name: cryptolake-streaming-producer
    environment:
      <<: *common-env
      PYTHONUNBUFFERED: "1"
      PYTHONPATH: /app
    command: python src/ingestion/streaming/binance_producer.py
    depends_on:
      - kafka
    volumes:
      - ./src:/app/src

  spark-streaming-bronze:
    build:
      context: ./docker/spark
      dockerfile: Dockerfile
    container_name: cryptolake-spark-bronze
    environment:
      <<: *common-env
      SPARK_MASTER_URL: spark://spark-master:7077
      PYTHONPATH: /opt/spark/work
    command: >
      /opt/spark/bin/spark-submit  --master spark://spark-master:7077  --conf spark.cores.max=1 --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0,org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.5.0  /opt/spark/work/src/processing/streaming/stream_to_bronze.py
    depends_on:
      - spark-master
      - kafka
      - iceberg-rest
    volumes:
      - ./src:/opt/spark/work/src

  spark-streaming-vwap:
    build:
      context: ./docker/spark
      dockerfile: Dockerfile
    container_name: cryptolake-spark-vwap
    environment:
      <<: *common-env
      SPARK_MASTER_URL: spark://spark-master:7077
      PYTHONPATH: /opt/spark/work
    command: >
      /opt/spark/bin/spark-submit  --master spark://spark-master:7077  --conf spark.cores.max=1 --packages org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.5.0,org.apache.hadoop:hadoop-aws:3.3.4 /opt/spark/work/src/processing/streaming/silver_realtime_vwap.py
    depends_on:
      - spark-master
      - iceberg-rest
      - spark-streaming-bronze
    volumes:
      - ./src:/opt/spark/work/src

  ml-inference:
    build:
      context: ./docker/api
      dockerfile: Dockerfile
    container_name: cryptolake-ml
    environment:
      <<: *common-env
      PYTHONPATH: /app
      PYTHONUNBUFFERED: "1"
    command: python src/ml/inference_service.py
    depends_on:
      - redis
      - api
    volumes:
      - ./src:/app/src
      - ./models:/app/models

  # ==========================================================
  # CAPA DE ORQUESTACIÓN
  # ==========================================================

  airflow-postgres:
    image: postgres:16-alpine
    container_name: cryptolake-airflow-db
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow
    volumes:
      - airflow-db-data:/var/lib/postgresql/data
    healthcheck:
      test: [ "CMD-SHELL", "pg_isready -U airflow" ]
      interval: 5s
      timeout: 5s
      retries: 5

  airflow-webserver:
    build:
      context: ./docker/airflow
      dockerfile: Dockerfile
    container_name: cryptolake-airflow-webserver
    ports:
      - "8083:8080"
    environment:
      <<: *common-env
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@airflow-postgres/airflow
      AIRFLOW__CORE__FERNET_KEY: ''
      AIRFLOW__WEBSERVER__SECRET_KEY: cryptolake-secret-key
      AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: 'true'
      AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
      PYTHONPATH: /opt/airflow
    volumes:
      - ./src/orchestration/dags:/opt/airflow/dags
      - ./src:/opt/airflow/src
      - airflow-logs:/opt/airflow/logs
    depends_on:
      airflow-postgres:
        condition: service_healthy
    command: bash -c "airflow db migrate && airflow users create --username admin --password admin --firstname Admin --lastname User --role Admin --email admin@cryptolake.dev || true && airflow webserver"

  airflow-scheduler:
    build:
      context: ./docker/airflow
      dockerfile: Dockerfile
    container_name: cryptolake-airflow-scheduler
    environment:
      <<: *common-env
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@airflow-postgres/airflow
      AIRFLOW__CORE__FERNET_KEY: ''
      AIRFLOW__WEBSERVER__SECRET_KEY: cryptolake-secret-key
      AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
      PYTHONPATH: /opt/airflow
    volumes:
      - ./src/orchestration/dags:/opt/airflow/dags
      - ./src:/opt/airflow/src
      - airflow-logs:/opt/airflow/logs
    depends_on:
      airflow-postgres:
        condition: service_healthy
    command: airflow scheduler

  # ==========================================================
  # CAPA DE SERVING
  # ==========================================================

  api:
    build:
      context: ./docker/api
      dockerfile: Dockerfile
    container_name: cryptolake-api
    ports:
      - "8000:8000"
    environment:
      <<: *common-env
      PYTHONUNBUFFERED: "1"
      PYTHONPATH: /app
    volumes:
      - ./src:/app/src
      - ./models:/app/models

  dashboard:
    image: python:3.11-slim
    container_name: cryptolake-dashboard
    ports:
      - "8501:8501"
    environment:
      <<: *common-env
      API_URL: http://api:8000
    volumes:
      - ./src/serving/dashboard:/app
    command: >
      bash -c "pip install streamlit requests plotly pandas streamlit-autorefresh && streamlit run /app/app.py --server.address 0.0.0.0"
    depends_on:
      - api

  # ==========================================================
  # MONITORING
  # ==========================================================

  prometheus:
    image: prom/prometheus:latest
    container_name: cryptolake-prometheus
    ports:
      - "9090:9090"
    volumes:
      - ./docker/prometheus/prometheus.yml:/etc/prometheus/prometheus.yml

  grafana:
    image: grafana/grafana:latest
    container_name: cryptolake-grafana
    ports:
      - "3000:3000"
    environment:
      GF_SECURITY_ADMIN_PASSWORD: cryptolake
    volumes:
      - grafana-data:/var/lib/grafana

volumes:
  minio-data:
  kafka-data:
  airflow-db-data:
  airflow-logs:
  grafana-data:
