# ğŸ”ï¸ CryptoLake â€” Real-Time Crypto Analytics Lakehouse

[![CI Pipeline](https://github.com/Weryyy/cryptolake-con-Copilot/actions/workflows/ci.yml/badge.svg)](https://github.com/Weryyy/cryptolake-con-Copilot/actions/workflows/ci.yml)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)
[![Python 3.11+](https://img.shields.io/badge/python-3.11+-blue.svg)](https://www.python.org/downloads/)
[![dbt](https://img.shields.io/badge/dbt-1.8-FF694B?logo=dbt)](https://www.getdbt.com/)
[![Apache Iceberg](https://img.shields.io/badge/Apache_Iceberg-1.5-blue)](https://iceberg.apache.org/)

> An end-to-end data engineering platform that ingests real-time and historical
> cryptocurrency data, processes it through a **Medallion Architecture** (Bronze â†’ Silver â†’ Gold)
> on **Apache Iceberg**, transforms with **dbt**, orchestrates with **Airflow**, and serves
> analytics via **REST API** and interactive **dashboard** â€” all containerized with Docker.

---

## ï¿½ Objetivo del Proyecto
El objetivo principal de **CryptoLake** es proporcionar una plataforma de datos robusta, escalable y de baja latencia para el anÃ¡lisis del mercado de criptomonedas. El proyecto demuestra la implementaciÃ³n de patrones modernos de ingenierÃ­a de datos, integrando:
*   **Ingesta HÃ­brida**: Captura de eventos en tiempo real (Binance) y lotes histÃ³ricos (CoinGecko).
*   **Eficiencia de Almacenamiento**: Uso de **Apache Iceberg** para manejar transacciones ACID, evoluciÃ³n de esquemas y compactaciÃ³n de datos.
*   **Gobernanza y Calidad**: Transformaciones estructuradas con **dbt** y validaciones de calidad en cada capa.
*   **Servicio de Datos**: ProvisiÃ³n de mÃ©tricas refinadas a travÃ©s de una API de alto rendimiento lista para ser consumida por aplicaciones finales.

---

## ï¿½ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         DATA SOURCES                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ Binance WS   â”‚  â”‚ CoinGecko    â”‚  â”‚ Alternative.me            â”‚  â”‚
â”‚  â”‚ (Real-time)  â”‚  â”‚ (Historical) â”‚  â”‚ (Fear & Greed Index)      â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚                 â”‚                        â”‚
          â–¼                 â–¼                        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   KAFKA          â”‚  â”‚   PYTHON EXTRACTORS                  â”‚
â”‚   (Streaming)    â”‚  â”‚   (Batch via Airflow)                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                          â”‚
         â–¼                          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 LAKEHOUSE (MinIO + Apache Iceberg)                    â”‚
â”‚                                                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ ğŸ¥‰ BRONZE   â”‚    â”‚ ğŸ¥ˆ SILVER    â”‚    â”‚ ğŸ¥‡ GOLD               â”‚  â”‚
â”‚  â”‚ (Raw)       â”‚â”€â”€â”€â–¶â”‚ (Cleaned)    â”‚â”€â”€â”€â–¶â”‚ (Star Schema)         â”‚  â”‚
â”‚  â”‚ Iceberg     â”‚    â”‚ Iceberg      â”‚    â”‚ Iceberg + dbt         â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚         â–²                  â–²                       â–²                 â”‚
â”‚    Spark Streaming    Spark Batch              dbt models            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”
                     â–¼                 â–¼
           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
           â”‚  FastAPI      â”‚  â”‚  Streamlit        â”‚
           â”‚  REST API     â”‚  â”‚  Dashboard        â”‚
           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ› ï¸ Tech Stack

| Layer | Technology | Purpose |
|-------|-----------|---------|
| **Streaming** | Apache Kafka | Real-time price ingestion from Binance |
| **Processing** | Apache Spark (PySpark) | Batch + stream processing |
| **Table Format** | Apache Iceberg | ACID transactions, time travel, schema evolution |
| **Storage** | MinIO (S3-compatible) | Object storage for Lakehouse |
| **Transformation** | dbt-core + dbt-spark | SQL-based dimensional modeling (Kimball) |
| **Orchestration** | Apache Airflow | Pipeline scheduling and monitoring |
| **Data Quality** | Great Expectations | Automated data validation |
| **API** | FastAPI | REST API for analytics |
| **Dashboard** | Streamlit | Interactive visualizations |
| **Containers** | Docker + Docker Compose | Reproducible deployment |
| **IaC** | Terraform | Infrastructure as Code |
| **CI/CD** | GitHub Actions | Automated testing and deployment |
| **Monitoring** | Prometheus + Grafana | Pipeline observability |
| **Code Quality** | Ruff + mypy + pre-commit | Linting + type checking |

## ğŸš€ Quick Start

```bash
# Clone the repository
git clone https://github.com/Weryyy/cryptolake-con-Copilot.git
cd cryptolake-con-Copilot

# Copy environment variables
cp .env.example .env

# Start all 12+ services with one command
make up

# Create Kafka topics
make kafka-create-topics

# Verify everything is running
python scripts/health_check.py
```

## ğŸ§  Machine Learning: Dual Memory Council

CryptoLake utiliza un enfoque de **Consejo de Agentes** basado en **Temporal Fusion Transformers (TFT)** con dos modelos de entrenamiento:

1.  **Memoria HistÃ³rica** (`--mode historical`): 200 Ã©pocas. Estabilidad macro.
2.  **Memoria Reciente** (`--mode recent`): 400 Ã©pocas. Sensibilidad micro (volatilidad).

Los modelos son **multivariados** (utilizan Precio + Volumen) y se sincronizan automÃ¡ticamente con el contenedor de la API mediante volÃºmenes de Docker.

### Services Dashboard

| Service | URL | Credentials |
|---------|-----|-------------|
| **MinIO Console** | http://localhost:9001 | `cryptolake` / `cryptolake123` |
| **Kafka UI** | http://localhost:8080 | â€” |
| **Spark UI** | http://localhost:8082 | â€” |
| **Airflow** | http://localhost:8083 | `admin` / `admin` |
| **API Docs** | http://localhost:8000/docs | â€” |
| **Dashboard** | http://localhost:8501 | â€” |
| **Grafana** | http://localhost:3000 | `admin` / `cryptolake` |

## ğŸ“Š Data Model

### Medallion Architecture

| Layer | Content | Format | Processing |
|-------|---------|--------|------------|
| **Bronze** | Raw data, unmodified | Iceberg (append-only) | Spark Streaming + Batch |
| **Silver** | Cleaned, deduplicated, typed | Iceberg (merge) | Spark Batch |
| **Gold** | Dimensional model (star schema) | Iceberg | dbt |

### Star Schema (Gold Layer)

- **`fact_market_daily`** â€” Daily crypto market metrics (price, volume, MAs, sentiment)
- **`fact_price_hourly`** â€” Hourly OHLCV from streaming data
- **`dim_coins`** â€” Cryptocurrency metadata and statistics
- **`dim_dates`** â€” Calendar dimension

## ğŸ“ˆ Key Features

- **Dual Pipeline**: Real-time streaming (Kafka â†’ Spark Streaming) + daily batch
- **Lakehouse Architecture**: Apache Iceberg with Medallion pattern (Bronze â†’ Silver â†’ Gold)
- **Dimensional Modeling**: Kimball star schema with facts and dimensions
- **Data Contracts**: Schema versioning and quality agreements between layers
- **Incremental Processing**: `MERGE INTO` for efficient Silver layer updates
- **Data Quality Gates**: Great Expectations validation suites
- **Production-Ready**: CI/CD, monitoring, alerting, structured logging

## ğŸ—‚ï¸ Project Structure

```
cryptolake/
â”œâ”€â”€ .github/workflows/       # CI/CD pipelines
â”œâ”€â”€ docker/                  # Dockerfiles (Spark, Airflow, API)
â”œâ”€â”€ terraform/               # Infrastructure as Code
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ config/              # Centralized settings (Pydantic)
â”‚   â”œâ”€â”€ ingestion/           # Streaming (Kafka) + Batch extractors
â”‚   â”œâ”€â”€ processing/          # Spark jobs (Bronze, Silver)
â”‚   â”œâ”€â”€ transformation/      # dbt models (Gold layer)
â”‚   â”œâ”€â”€ orchestration/       # Airflow DAGs
â”‚   â”œâ”€â”€ quality/             # Great Expectations suites
â”‚   â””â”€â”€ serving/             # FastAPI + Streamlit
â”œâ”€â”€ tests/                   # Unit + Integration tests
â”œâ”€â”€ docs/                    # Architecture, data dictionary, contracts
â”œâ”€â”€ scripts/                 # Setup and utility scripts
â”œâ”€â”€ docker-compose.yml       # Full local environment
â”œâ”€â”€ Makefile                 # Developer commands
â””â”€â”€ pyproject.toml           # Python project configuration
```

## ğŸ§ª Development

```bash
# Create virtual environment
python3.11 -m venv .venv
source .venv/bin/activate

# Install dependencies (including dev tools)
pip install -e ".[dev]"

# Run tests
make test

# Run linting
make lint

# Format code
make format

# Run the full pipeline manually
make pipeline
```

## ğŸ“ Documentation

- [Architecture Decision Records](docs/architecture.md)
- [Data Dictionary](docs/data_dictionary.md)
- [Setup Guide](docs/setup_guide.md)
- [Data Contracts](docs/data_contracts/)
- [Troubleshooting Log](troubleshooting_log.md)

---

## ğŸš€ Roadmap y Optimizaciones
Estamos evolucionando el proyecto con las siguientes mejoras crÃ­ticas:

### 1. OptimizaciÃ³n del Almacenamiento (Iceberg Tuning)
*   **Hidden Partitioning & Sort Orders**: ImplementaciÃ³n de `SORTED BY (timestamp)` en archivos Iceberg para maximizar el *data skipping* con PyArrow.
*   **Compaction DAG**: AutomatizaciÃ³n con Airflow para ejecutar `rewriteDataFiles`, consolidando micro-archivos de streaming en archivos optimizados.

### 2. Algoritmos de Rendimiento
*   **VWAP en Tiempo Real**: CÃ¡lculo distribuido del precio promedio ponderado por volumen en ventanas deslizantes.
*   **DetecciÃ³n de AnomalÃ­as**: Capa de QA que utiliza Z-Score para identificar y marcar variaciones sospechosas en tiempo real.

### 3. Analytics Avanzado (Gold Layer)
*   **Modelos OHLC**: Agregaciones dbt para velas de 1h, 4h y 1d directamente en la capa Gold.
*   **API Hot-Path**: MigraciÃ³n de las consultas pesadas del dashboard a tablas Gold pre-agregadas.

### 4. CachÃ© de Baja Latencia
*   **Redis Integration**: Almacenamiento en cachÃ© de los "Ãºltimos 5 minutos" de precios para reducir la carga sobre el Storage Layer y permitir una respuesta de API sub-10ms.

---

## ğŸ“œ License

This project is licensed under the MIT License â€” see the [LICENSE](LICENSE) file for details.
